{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dd_logo.png\" />\n",
    "\n",
    "# Distributed Tracing with Datadog APM\n",
    "\n",
    "Now that we've got the basics for how traces work, it's time to trace our first distributed system. \n",
    "\n",
    "In our case, we'll use `docker-compose` to load up two Flask APIs, an Agent, and a redis server. We'll first manually instrument our application, and then see how to enable the distributed tracer in Datadog to automatically instrument our API.\n",
    "\n",
    "Before we get started, be sure to check out the repo that goes along with this. It'll have everything you need.\n",
    "\n",
    "If you're running this notebook locally, you should already be good. Otherwise, you'll want to:\n",
    "\n",
    "```bash\n",
    "$ git clone https://github.com/burningion/dash-apm-workshop\n",
    "$ cd dash-apm-workshop\n",
    "$ jupyter notebook\n",
    "```\n",
    "\n",
    "Finally, stop running the existing Datadog Agent container if you're continuing from the Quickstart.\n",
    "\n",
    "You can do this via a:\n",
    "\n",
    "```bash\n",
    "$ docker ps\n",
    "$ docker kill <PSNAMEOFAGENT>\n",
    "```\n",
    "\n",
    "Once you've stopped the Datadog Agent container, you'll be able to move on to the example project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Becoming Acquainted with the Example Project\n",
    "\n",
    "<img src=\"images/architecture.png\" />\n",
    "\n",
    "Before we instrument our example project, let's become familiar with its architecture.\n",
    "\n",
    "Our example is an API, that sends of requests to a `thinker` microservice. It runs via a `docker-compose.yml` file, that spins up four containers.\n",
    "\n",
    "Right now, we have two Flask apps (think api and thinker service), an instance of the Datadog Agent container, and a redis container.\n",
    "\n",
    "Requests flow from the Think API to the Thinker microservice, and the redis instance is not yet hooked up to anything. We'll edit our code in a later step, and use it as a datastore.\n",
    "\n",
    "The Datadog Agent is set up to receive traces at its default 8126 port.\n",
    "\n",
    "Open up a new terminal, and spin up the docker-compose of the repo:\n",
    "\n",
    "```bash\n",
    "$ DD_API_KEY=<YOUR_API_KEY> STEP=1 docker-compose up\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Test Requests to our API\n",
    "Running docker-compose up spins up all the containers for our infrastructure. \n",
    "\n",
    "In this case, we're using docker-compose to spin up two microservices. For now, we've got an API that sits in front of our microservice, and of course, a microservice. \n",
    "\n",
    "The first example is already set up with a basic tracer initialized, so by putting in our key, we can already see traces being sent.\n",
    "\n",
    "Let's try our first `curl` request to the API, and see if we can trace our request across both services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=mankind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Default Traces  Across Systems\n",
    "Now that a few requests have been sent, we can take a look at the Datadog APM dashboard, and see what's going on with our service.\n",
    "\n",
    "<img src=\"images/first-thinker-api.png\" />\n",
    "\n",
    "Looking at the dashboard, it appears our trace which should be a single trace is broken out into two separate traces.\n",
    "\n",
    "<img src=\"images/first-thinker-micro.png\" />\n",
    "\n",
    "Our customer facing API is hitting the `thinker` microservice, but the trace coming from the `api` service isn't being propagated across both.\n",
    "\n",
    "By default, Datadog's APM implementation doesn't send or look for the request headers that would go across applications. \n",
    "\n",
    "This is because traces allow you to pass along potentially private information. It's better if we only pass the headers of our trace along to infrastructure that we know is our own.\n",
    "\n",
    "Let's walk through adding our trace headers to our APIs, first manually, and then automatically with the `distributed_tracing` flag.\n",
    "\n",
    "\n",
    "## Manually Continuing Our Trace Across Systems\n",
    "\n",
    "If we look at the `thinker.py` file, we can see that even though our `think` function is wrapped in a trace, we're not continuing or checking for any exisisting spans. \n",
    "\n",
    "In order to do that within Flask, we'll need to add `X-Datadog-Trace-Id` and `X-Datadog-Parent-Id` to our requests that go into our private `thinker` API, injecting our `trace_id` and `parent_id`.\n",
    "\n",
    "Once our request headers make it to the private `thinker` service, we then check to see if they exist, and add them into our current span context.\n",
    "\n",
    "Our Python code for the `thinker` service becomes the following:\n",
    "\n",
    "```python\n",
    "@app.route('/')\n",
    "def think_microservice():\n",
    "    # continue the span from the called service\n",
    "    trace_id = flask_request.headers.get(\"X-Datadog-Trace-Id\")\n",
    "    parent_id = flask_request.headers.get(\"X-Datadog-Parent-Id\")\n",
    "    if trace_id and parent_id:\n",
    "        span = tracer.current_span()\n",
    "        span.trace_id = int(trace_id)\n",
    "        span.parent_id = int(parent_id)\n",
    "\n",
    "    subject = flask_request.args.get('subject')\n",
    "    thoughts = think(subject)\n",
    "    return Response(thoughts, mimetype='application/json')\n",
    "```\n",
    "\n",
    "Notice the `think` function that gets called has a Python decorator. It's wrapping the function call with a span, and inserting the `subject` of the think call into the span's `tag`:\n",
    "\n",
    "\n",
    "```python\n",
    "@tracer.wrap(name='think')\n",
    "def think(subject):\n",
    "    tracer.current_span().set_tag('subject', subject)\n",
    "\n",
    "    sleep(0.5)\n",
    "    return thoughts[subject]\n",
    "```\n",
    "\n",
    "Going back to our original `API` application, we also need to instrument and send our trace information in the part where we make our web request:\n",
    "\n",
    "```python\n",
    "@app.route('/think/')\n",
    "def think_handler():\n",
    "    thoughts = requests.get('http://thinker:5001/', headers={\n",
    "        'x-datadog-trace-id': str(tracer.current_span().trace_id),\n",
    "        'x-datadog-parent-id': str(tracer.current_span().span_id),\n",
    "    }, params={\n",
    "        'subject': flask_request.args.getlist('subject', str),\n",
    "    }).content\n",
    "    return Response(thoughts, mimetype='application/json')\n",
    "```\n",
    "\n",
    "If we want, we can restart our containers now, and see how things look with requests being passed across services:\n",
    "\n",
    "```bash\n",
    "$ docker-compose down\n",
    "$ DD_API_KEY=<YOUR_API_KEY> STEP=2 docker-compose up\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Cross Service Spans\n",
    "\n",
    "In order to view our cross service spans, we'll first need to generate some more requests, creating new traces to be sent back to Datadog.\n",
    "\n",
    "Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=mankind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try generating an error in our application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=peace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we switch over to view our traces in Datadog, we see them coming in as a single span, traversing our microservices.\n",
    "\n",
    "<img src=\"images/second-thinker-api.png\" />\n",
    "\n",
    "But if you looked closely, you'll see that we have a library that can be instrumented by Datadog, but isn't.\n",
    "\n",
    "That's the `requests` library, that's used to send our requests across from one microservice to the other. \n",
    "\n",
    "## Automatic Distributed Tracing \n",
    "\n",
    "Now that we've seen how to manually add distributed tracing headers to our internal infrastructure, let's set things up the easy way.\n",
    "\n",
    "If you're following along with the code, we're now in `step03`.\n",
    "\n",
    "We can add automatic distributed tracing to Datadog [supported libraries](https://docs.datadoghq.com/tracing/setup/python/#compatibility) by adding a simple `distributed_tracing=True` to our `TraceMiddleware`.\n",
    "\n",
    "This adds checks for the headers from before, and automatically continues as a child span where necessary.\n",
    "\n",
    "If we use Datadog's Python library function `patch`, we can also automatically instrument the `requests` library, along with the `redis` server we have running.\n",
    "\n",
    "To send our headers along with the automatically instrumented `requests` library, we must also import `config` from `ddtrace`, and add the following lines:\n",
    "\n",
    "```python\n",
    "from ddtrace import tracer, patch, config\n",
    "\n",
    "# Tracer configuration\n",
    "tracer.configure(hostname='agent')\n",
    "patch(requests=True)\n",
    "\n",
    "# enable distributed tracing for requests\n",
    "# to send headers (globally)\n",
    "config.requests['distributed_tracing'] = True\n",
    "```\n",
    "\n",
    "By using the Datadog patch, we get more default metadata of our request along with the information set.\n",
    "\n",
    "Now we can see our traces as they propagate across our entire distributed system.\n",
    "\n",
    "<img src=\"images/automatic-distributed.png\" />\n",
    "\n",
    "But we're still running a simplified system. Let's add a datastore and see how that changes what distributed tracing shows us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding and Instrumenting a Datastore\n",
    "\n",
    "If we open up the `step04` folder, we can see how we've added a SQLite datastore to the infrastructure of our code.\n",
    "\n",
    "Now, instead of using a `namedtuple`, we use a [SQLAlchemy](http://www.sqlalchemy.org/) backend, instrumented in a `bootstrap.py` file, and a formal `models.py` file too. Let's open these up, and see what things look like.\n",
    "\n",
    "*`step04/models.py`*:\n",
    "\n",
    "```python\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "\n",
    "\n",
    "# don't initialize the SQLAlchemy immediately\n",
    "db = SQLAlchemy()\n",
    "\n",
    "\n",
    "class Thought(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    quote = db.Column(db.String(128), unique=True)\n",
    "    author = db.Column(db.String(32))\n",
    "\n",
    "    subject = db.Column(db.String(32))\n",
    "\n",
    "    def __init__(self, quote, author, subject):\n",
    "        self.quote = quote\n",
    "        self.author = author\n",
    "        self.subject = subject\n",
    "\n",
    "    def serialize(self):\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'quote': self.quote,\n",
    "            'author': self.author,\n",
    "            'subject': self.subject\n",
    "        }\n",
    "```\n",
    "\n",
    "*`step04/bootstrap.py`*:\n",
    "```python\n",
    "from flask import Flask\n",
    "from ddtrace import tracer, patch\n",
    "patch(sqlalchemy=True,sqlite3=True)\n",
    "from models import Thought, db\n",
    "\n",
    "\n",
    "# configure the tracer so that it reaches the Datadog Agent\n",
    "# available in another container\n",
    "tracer.configure(hostname='agent')\n",
    "\n",
    "\n",
    "def create_app():\n",
    "    \"\"\"Create a Flask application\"\"\"\n",
    "    app = Flask(__name__)\n",
    "    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///app.db'\n",
    "    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n",
    "\n",
    "    db.init_app(app)\n",
    "    initialize_database(app, db)\n",
    "    return app\n",
    "\n",
    "\n",
    "def initialize_database(app, db):\n",
    "    \"\"\"Drop and restore database in a consistent state\"\"\"\n",
    "    with app.app_context():\n",
    "        db.drop_all()\n",
    "        db.create_all()\n",
    "\n",
    "        db.session.add(Thought(quote='My religion consists of a humble admiration of the illimitable superior spirit who reveals himself in the slight details we are able to perceive with our frail and feeble mind.',\n",
    "                               author='Albert Einstein',\n",
    "                               subject='religion'))\n",
    "\n",
    "        db.session.add(Thought(quote='For a successful technology, reality must take precedence over public relations, for Nature cannot be fooled.',\n",
    "                               author='Richard Feynman',\n",
    "                               subject='technology'))\n",
    "        db.session.add(Thought(quote='One is left with the horrible feeling now that war settles nothing; that to win a war is as disastrous as to lose one.',\n",
    "                               author='Agatha Christie',\n",
    "                               subject='war'))\n",
    "        db.session.add(Thought(quote='Life grants nothing to us mortals without hard work.',\n",
    "                               author='Horace',\n",
    "                               subject='work'))\n",
    "        db.session.add(Thought(quote='Ah, music. A magic beyond all we do here!',\n",
    "                               author='J. K. Rowling',\n",
    "                               subject='music'))\n",
    "        db.session.add(Thought(quote='I think that God in creating Man somewhat overestimated his ability.',\n",
    "                               author='Oscar Wilde',\n",
    "                               subject='mankind'))\n",
    "        db.session.commit()\n",
    "\n",
    "```\n",
    "\n",
    "Notice how we just added a `patch()` to our model, and defined where on the network the Datadog Agent lives.\n",
    "\n",
    "With this small addition of code, we've visual feedback on how our ORM system works, and where the bottlenecks might be.\n",
    "\n",
    "Even though we've been using a contrived example to demonstrate how to instrument traces, let's add a layer of cache next and see how it speeds up our processes, and how tracing allows for transparency in our caching systems too.\n",
    "\n",
    "If you're following along with the code, you can now spin up the docker-compose with `STEP=4`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=mankind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=war"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, let's raise an error in our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the results generated back in the Datadog APM interface, and see how our requests have now propagated:\n",
    "\n",
    "<img src=\"images/serialize-error.png\" />\n",
    "\n",
    "Besides seeing our requests propagated in the trace list, notice how we can see exactly where our errors have been raised in our distributed system. \n",
    "\n",
    "The error is happening in the `thinker` microservice, when we don't have a matching `Thought` for a subject. \n",
    "\n",
    "One fix could be adding a default thought for any subject. Maybe that's something we'll have to think about?\n",
    "\n",
    "Something like this should be a fix for our problematic code:\n",
    "\n",
    "```python\n",
    "@tracer.wrap(name='think')\n",
    "def think(subject):\n",
    "    tracer.current_span().set_tag('subject', subject)\n",
    "\n",
    "    sleep(0.5)\n",
    "    quote = Thought.query.filter_by(subject=subject).first()\n",
    "    \n",
    "    if quote is None:\n",
    "        return Thought(quote='Hmmm, that\\'s something I\\'ll need to think about.',\n",
    "                       author='The Machine',\n",
    "                       subject=subject)\n",
    "    return quote\n",
    "\n",
    "```\n",
    "\n",
    "Let's try running that now, and see whether it's a fix for our error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Cache Layer\n",
    "\n",
    "Right now, our app uses a `sleep` inside the `think` function. \n",
    "\n",
    "In order to simulate caching, let's add a `@cache.memoize()` function decorator around the `think` function itself.\n",
    "\n",
    "This will end up storing the value sent to the function, along with the result generated by the function, for 30 seconds at a time.\n",
    "\n",
    "By adding this cache, we should see a slow first request, followed by fast requests afterwards.\n",
    "\n",
    "For the instrumentation, we'll use the [`Flask-Caching`](https://pypi.org/project/Flask-Caching/) library, and hook it up to the redis container we've got running. \n",
    "\n",
    "Our code, fully instrumented, looks like this:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "from flask import Flask, Response, jsonify\n",
    "from flask import request as flask_request\n",
    "\n",
    "from flask_caching import Cache\n",
    "\n",
    "from ddtrace import tracer, patch\n",
    "from ddtrace.contrib.flask import TraceMiddleware\n",
    "\n",
    "from bootstrap import create_app\n",
    "from models import Thought\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "patch(redis=True)\n",
    "app = create_app()\n",
    "cache = Cache(config={'CACHE_TYPE': 'redis', 'CACHE_REDIS_HOST': 'redis'})\n",
    "cache.init_app(app)\n",
    "\n",
    "traced_app = TraceMiddleware(app, tracer, service='thinker-microservice', distributed_tracing=True)\n",
    "\n",
    "# Tracer configuration\n",
    "tracer.configure(hostname='agent')\n",
    "\n",
    "@tracer.wrap(name='think')\n",
    "@cache.memoize(30)\n",
    "def think(subject):\n",
    "    tracer.current_span().set_tag('subject', subject)\n",
    "\n",
    "    sleep(0.5)\n",
    "    quote = Thought.query.filter_by(subject=subject).first()\n",
    "    \n",
    "    return quote\n",
    "\n",
    "@app.route('/')\n",
    "def think_microservice():\n",
    "    # because we have distributed tracing, don't need to manually grab headers\n",
    "    subject = flask_request.args.get('subject')\n",
    "    thoughts = think(subject)\n",
    "    return jsonify(thoughts.serialize())\n",
    "```\n",
    "\n",
    "As my favorite side effect of instrumenting, by running some tests through our code again, we can now see how the `Flask-Caching` library implements its caches through redis:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=mankind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=mankind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=war"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, running the same cell twice in under 30 seconds should be generated much quickly than the first request.\n",
    "\n",
    "And indeed, looking at the APM backend, we can see the changes with a proper cache hit. From 504ms down to 4.63ms:\n",
    "\n",
    "<img src=\"images/cache-miss.png\" />\n",
    "\n",
    "<img src=\"images/cache-hit.png\" />\n",
    "\n",
    "The product owners are going to love that. And if there's an issue that pops up with caching and edge cases, we'll be able to see the exact data and requests which triggered our responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here?\n",
    "\n",
    "Bringing into your organization, other repositories with example code.\n",
    "\n",
    "Once again, the great work done by Andrew McBurney with Homebrew while an intern at Datadog. Great use case of using Tracing to instrument a monolithic application:\n",
    "\n",
    "https://www.datadoghq.com/blog/engineering/using-datadog-apm-to-find-bottlenecks-and-performance-benchmarking/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
