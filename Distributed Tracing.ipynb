{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dd_logo.png\" />\n",
    "\n",
    "# Distributed Tracing with Datadog APM\n",
    "\n",
    "Now that we've got the basics for how traces work, it's time to trace our first distributed system. \n",
    "\n",
    "In our case, we'll use `docker-compose` to load up two Flask APIs, an Agent, and a redis server. We'll first manually instrument our application, and then see how to enable the distributed tracer in Datadog to automatically instrument our API.\n",
    "\n",
    "Before we get started, be sure to check out the repo that goes along with this. It'll have everything you need.\n",
    "\n",
    "If you're running this notebook locally, you should already be good. Otherwise, you'll want to:\n",
    "\n",
    "```bash\n",
    "$ git clone https://github.com/burningion/dash-apm-workshop\n",
    "$ cd dash-apm-workshop\n",
    "$ jupyter notebook\n",
    "```\n",
    "\n",
    "Finally, stop running the existing Datadog Agent container if you're continuing from the Quickstart.\n",
    "\n",
    "You can do this via a:\n",
    "\n",
    "```bash\n",
    "$ docker ps\n",
    "$ docker kill <PSNAMEOFAGENT>\n",
    "```\n",
    "\n",
    "Once you've stopped the Datadog Agent container, you'll be able to move on to the example project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Becoming Acquainted with the Example Project\n",
    "\n",
    "<img src=\"images/architecture.png\" />\n",
    "\n",
    "Before we instrument our example project, let's become familiar with its architecture.\n",
    "\n",
    "Our example is an API, that sends of requests to a `thinker` microservice. It runs via a `docker-compose.yml` file, that spins up four containers.\n",
    "\n",
    "Right now, we have two Flask apps (think api and thinker service), an instance of the Datadog Agent container, and a redis container.\n",
    "\n",
    "Requests flow from the Think API to the Thinker microservice, and the redis instance is not yet hooked up to anything. We'll edit our code in a later step, and use it as a datastore.\n",
    "\n",
    "The Datadog Agent is set up to receive traces at its default 8126 port.\n",
    "\n",
    "Open up a new terminal, and spin up the docker-compose of the repo:\n",
    "\n",
    "```bash\n",
    "$ DD_API_KEY=<YOUR_API_KEY> STEP=1 docker-compose up\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Test Requests to our API\n",
    "Running docker-compose up spins up all the containers for our infrastructure. \n",
    "\n",
    "In this case, we're using docker-compose to spin up two microservices. For now, we've got an API that sits in front of our microservice, and of course, a microservice. \n",
    "\n",
    "The first example is already set up with a basic tracer initialized, so by putting in our key, we can already see traces being sent.\n",
    "\n",
    "Let's try our first `curl` request to the API, and see if we can trace our request across both services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=mankind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Default Traces  Across Systems\n",
    "Now that a few requests have been sent, we can take a look at the Datadog APM dashboard, and see what's going on with our service.\n",
    "\n",
    "<img src=\"images/first-thinker-api.png\" />\n",
    "\n",
    "Looking at the dashboard, it appears our trace which should be a single trace is broken out into two separate traces.\n",
    "\n",
    "<img src=\"images/first-thinker-micro.png\" />\n",
    "\n",
    "Our customer facing API is hitting the `thinker` microservice, but the trace coming from the `api` service isn't being propagated across both.\n",
    "\n",
    "By default, Datadog's APM implementation doesn't send or look for the request headers that would go across applications. \n",
    "\n",
    "This is because traces allow you to pass along potentially private information. It's better if we only pass the headers of our trace along to infrastructure that we know is our own.\n",
    "\n",
    "Let's walk through adding our trace headers to our APIs, first manually, and then automatically with the `distributed_tracing` flag.\n",
    "\n",
    "\n",
    "## Manually Continuing Our Trace Across Systems\n",
    "\n",
    "If we look at the `thinker.py` file, we can see that even though our `think` function is wrapped in a trace, we're not continuing or checking for any exisisting spans. \n",
    "\n",
    "In order to do that within Flask, we'll need to add `X-Datadog-Trace-Id` and `X-Datadog-Parent-Id` to our requests that go into our private `thinker` API, injecting our `trace_id` and `parent_id`.\n",
    "\n",
    "Once our request headers make it to the private `thinker` service, we then check to see if they exist, and add them into our current span context.\n",
    "\n",
    "Our Python code for the `thinker` service becomes the following:\n",
    "\n",
    "```python\n",
    "@app.route('/')\n",
    "def think_microservice():\n",
    "    # continue the span from the called service\n",
    "    trace_id = flask_request.headers.get(\"X-Datadog-Trace-Id\")\n",
    "    parent_id = flask_request.headers.get(\"X-Datadog-Parent-Id\")\n",
    "    if trace_id and parent_id:\n",
    "        span = tracer.current_span()\n",
    "        span.trace_id = int(trace_id)\n",
    "        span.parent_id = int(parent_id)\n",
    "\n",
    "    subject = flask_request.args.get('subject')\n",
    "    thoughts = think(subject)\n",
    "    return Response(thoughts, mimetype='application/json')\n",
    "```\n",
    "\n",
    "Notice the `think` function that gets called has a Python decorator. It's wrapping the function call with a span, and inserting the `subject` of the think call into the span's `tag`:\n",
    "\n",
    "\n",
    "```python\n",
    "@tracer.wrap(name='think')\n",
    "def think(subject):\n",
    "    tracer.current_span().set_tag('subject', subject)\n",
    "\n",
    "    sleep(0.5)\n",
    "    return thoughts[subject]\n",
    "```\n",
    "\n",
    "Going back to our original `API` application, we also need to instrument and send our trace information in the part where we make our web request:\n",
    "\n",
    "```python\n",
    "@app.route('/think/')\n",
    "def think_handler():\n",
    "    thoughts = requests.get('http://thinker:5001/', headers={\n",
    "        'x-datadog-trace-id': str(tracer.current_span().trace_id),\n",
    "        'x-datadog-parent-id': str(tracer.current_span().span_id),\n",
    "    }, params={\n",
    "        'subject': flask_request.args.getlist('subject', str),\n",
    "    }).content\n",
    "    return Response(thoughts, mimetype='application/json')\n",
    "```\n",
    "\n",
    "If we want, we can restart our containers now, and see how things look with requests being passed across services:\n",
    "\n",
    "```bash\n",
    "$ docker-compose down\n",
    "$ DD_API_KEY=<YOUR_API_KEY> STEP=2 docker-compose up\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Cross Service Spans\n",
    "\n",
    "In order to view our cross service spans, we'll first need to generate some more requests, creating new traces to be sent back to Datadog.\n",
    "\n",
    "Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=mankind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try generating an error in our application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:5000/think/?subject=peace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we switch over to view our traces in Datadog, we see them coming in as a single span, traversing our microservices.\n",
    "\n",
    "<img src=\"images/second-thinker-api.png\" />\n",
    "\n",
    "But if you looked closely, you'll see that we have a library that can be instrumented by Datadog, but isn't.\n",
    "\n",
    "That's the `requests` library, that's used to send our requests across from one microservice to the other. \n",
    "\n",
    "## Automatic Distributed Tracing \n",
    "\n",
    "Now that we've seen how to manually add distributed tracing headers to our internal infrastructure, let's set things up the easy way.\n",
    "\n",
    "If you're following along with the code, we're now in `step03`.\n",
    "\n",
    "We can add automatic distributed tracing to Datadog [supported libraries](https://docs.datadoghq.com/tracing/setup/python/#compatibility) by adding a simple `distributed_tracing=True` to our `TraceMiddleware`.\n",
    "\n",
    "This adds checks for the headers from before, and automatically continues as a child span where necessary.\n",
    "\n",
    "If we use Datadog's Python library function `patch`, we can also automatically instrument the `requests` library, along with the `redis` server we have running.\n",
    "\n",
    "To send our headers along with the automatically instrumented `requests` library, we must also import `config` from `ddtrace`, and add the following lines:\n",
    "\n",
    "```python\n",
    "from ddtrace import tracer, patch, config\n",
    "\n",
    "# Tracer configuration\n",
    "tracer.configure(hostname='agent')\n",
    "patch(requests=True)\n",
    "\n",
    "# enable distributed tracing for requests\n",
    "# to send headers (globally)\n",
    "config.requests['distributed_tracing'] = True\n",
    "```\n",
    "\n",
    "By using the Datadog patch, we get more default metadata of our request along with the information set.\n",
    "\n",
    "Now we can see our traces as they propagate across our entire distributed system.\n",
    "\n",
    "<img src=\"images/automatic-distributed.png\" />\n",
    "\n",
    "But we're still running a simplified system. Let's add a datastore and see how that changes what distributed tracing shows us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding and Instrumenting a Datastore\n",
    "\n",
    "## Where to go from here?\n",
    "\n",
    "Bringing into your organization, other repositories with example code.\n",
    "\n",
    "Once again, the great work done by Andrew McBurney with Homebrew while an intern at Datadog. Great use case of using Tracing to instrument a monolithic application:\n",
    "\n",
    "https://www.datadoghq.com/blog/engineering/using-datadog-apm-to-find-bottlenecks-and-performance-benchmarking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
